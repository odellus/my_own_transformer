# **my_own_transformer**

A simple transformer block with vanilla self-attention, layer norm, and an MLP layer.

This is a toy. Not intended for anything other than wrapping your head around what a single sequence of input embeddings goes through inside a transformer block. Dropout, masking, and multi-headed attention are intentionally left aside to let us focus on the nuts and bolts of the math.